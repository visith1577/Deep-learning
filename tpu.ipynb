{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tpu.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP4bFA2EQjErBUcUnSDw0Lh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visith1577/Deep-learning/blob/main/tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjvvP9a1eZtT"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhOAqXjDdz-3"
      },
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except:\n",
        "  print(\"Naa\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNlYabPoekOy"
      },
      "source": [
        "SIZE = 224 #@param [\"192\", \"224\", \"331\", \"512\"] {type:\"raw\"}\n",
        "IMAGE_SIZE = [SIZE, SIZE]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdbPyjTgh57_"
      },
      "source": [
        "import random"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBGCeWtDhtzP",
        "outputId": "3b5569e7-b811-4d07-c910-148ca1204275"
      },
      "source": [
        "GCS_PATTERN = 'gs://flowers-public/tfrecords-jpeg-{}x{}/*.tfrec'.format(IMAGE_SIZE[0], IMAGE_SIZE[1])\n",
        "\n",
        "BATCH_SIZE = 128  # On TPU in Keras, this is the per-core batch size. The global batch size is 8x this.\n",
        "\n",
        "VALIDATION_SPLIT = 0.2\n",
        "CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # do not change, maps to the labels in the data (folder names)\n",
        "\n",
        "# splitting data files between training and validation\n",
        "filenames = tf.io.gfile.glob(GCS_PATTERN)\n",
        "random.shuffle(filenames)\n",
        "\n",
        "split = int(len(filenames) * VALIDATION_SPLIT)\n",
        "training_filenames = filenames[split:]\n",
        "validation_filenames = filenames[:split]\n",
        "print(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\n",
        "\n",
        "validation_steps = int(3670 // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
        "steps_per_epoch = int(3670 // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
        "print(\"With a batch size of {}, there will be {} batches per training epoch and {} batch(es) per validation run.\".format(BATCH_SIZE, steps_per_epoch, validation_steps))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pattern matches 16 data files. Splitting dataset into 13 training files and 3 validation files\n",
            "With a batch size of 128, there will be 23 batches per training epoch and 5 batch(es) per validation run.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wRpcAWQh3Lo"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "    features = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
        "        \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, features)\n",
        "    image = example['image']\n",
        "    class_label = example['class']\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    class_label = tf.cast(class_label, tf.int32)\n",
        "    return image, class_label\n",
        "\n",
        "def load_dataset(filenames):\n",
        "  # read from TFRecords. For optimal performance, use \"interleave(tf.data.TFRecordDataset, ...)\"\n",
        "  # to read from multiple TFRecord files at once and set the option experimental_deterministic = False\n",
        "  # to allow order-altering optimizations.\n",
        "\n",
        "  option_no_order = tf.data.Options()\n",
        "  option_no_order.experimental_deterministic = False\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
        "  dataset = dataset.with_options(option_no_order)\n",
        "  dataset = dataset.interleave(tf.data.TFRecordDataset, cycle_length=16, num_parallel_calls=AUTO) # faster\n",
        "  dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
        "  return dataset\n",
        "\n",
        "def get_batched_dataset(filenames):\n",
        "  dataset = load_dataset(filenames)\n",
        "  dataset = dataset.shuffle(2048)\n",
        "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=False) # drop_remainder will be needed on TPU\n",
        "  dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "  return dataset\n",
        "\n",
        "def get_training_dataset():\n",
        "  dataset = get_batched_dataset(training_filenames)\n",
        "  dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "  return dataset\n",
        "\n",
        "def get_validation_dataset():\n",
        "  dataset = get_batched_dataset(validation_filenames)\n",
        "  dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "  return dataset"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA3ELNIKiTfs"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, classes):\n",
        "    super(MyModel, self).__init__()\n",
        "    self._conv1a = tf.keras.layers.Conv2D(kernel_size=3, filters=16, padding='same', activation='relu')\n",
        "    self._conv1b = tf.keras.layers.Conv2D(kernel_size=3, filters=30, padding='same', activation='relu')\n",
        "    self._maxpool1 = tf.keras.layers.MaxPooling2D(pool_size=2)\n",
        "    \n",
        "    self._conv2a = tf.keras.layers.Conv2D(kernel_size=3, filters=60, padding='same', activation='relu')\n",
        "    self._maxpool2 = tf.keras.layers.MaxPooling2D(pool_size=2)\n",
        "    \n",
        "    self._conv3a = tf.keras.layers.Conv2D(kernel_size=3, filters=90, padding='same', activation='relu')\n",
        "    self._maxpool3 = tf.keras.layers.MaxPooling2D(pool_size=2)\n",
        "    \n",
        "    self._conv4a = tf.keras.layers.Conv2D(kernel_size=3, filters=110, padding='same', activation='relu')\n",
        "    self._maxpool4 = tf.keras.layers.MaxPooling2D(pool_size=2)\n",
        "    \n",
        "    self._conv5a = tf.keras.layers.Conv2D(kernel_size=3, filters=130, padding='same', activation='relu')\n",
        "    self._conv5b = tf.keras.layers.Conv2D(kernel_size=3, filters=40, padding='same', activation='relu')\n",
        "    \n",
        "    self._pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    self._classifier = tf.keras.layers.Dense(classes, activation='softmax')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self._conv1a(inputs)\n",
        "    x = self._conv1b(x)\n",
        "    x = self._maxpool1(x)\n",
        "\n",
        "    x = self._conv2a(x)\n",
        "    x = self._maxpool2(x)\n",
        "\n",
        "    x = self._conv3a(x)\n",
        "    x = self._maxpool3(x)\n",
        "\n",
        "    x = self._conv4a(x)\n",
        "    x = self._maxpool4(x)\n",
        "\n",
        "    x = self._conv5a(x)\n",
        "    x = self._conv5b(x)\n",
        "\n",
        "    x = self._pooling(x)\n",
        "    x = self._classifier(x)\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TSx-pA4iev1"
      },
      "source": [
        "def read_tfrecord(example):\n",
        "    features = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
        "        \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, features)\n",
        "    image = example['image']\n",
        "    class_label = example['class']\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    class_label = tf.cast(class_label, tf.int32)\n",
        "    return image, class_label\n",
        "\n",
        "def load_dataset(filenames):\n",
        "  # read from TFRecords. For optimal performance, use \"interleave(tf.data.TFRecordDataset, ...)\"\n",
        "  # to read from multiple TFRecord files at once and set the option experimental_deterministic = False\n",
        "  # to allow order-altering optimizations.\n",
        "\n",
        "  option_no_order = tf.data.Options()\n",
        "  option_no_order.experimental_deterministic = False\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
        "  dataset = dataset.with_options(option_no_order)\n",
        "  dataset = dataset.interleave(tf.data.TFRecordDataset, cycle_length=16, num_parallel_calls=AUTO) # faster\n",
        "  dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
        "  return dataset\n",
        "\n",
        "def get_batched_dataset(filenames):\n",
        "  dataset = load_dataset(filenames)\n",
        "  dataset = dataset.shuffle(2048)\n",
        "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=False) # drop_remainder will be needed on TPU\n",
        "  dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "  return dataset\n",
        "\n",
        "def get_training_dataset():\n",
        "  dataset = get_batched_dataset(training_filenames)\n",
        "  dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "  return dataset\n",
        "\n",
        "def get_validation_dataset():\n",
        "  dataset = get_batched_dataset(validation_filenames)\n",
        "  dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "  return dataset"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb0SLj_qi5_T"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, classes):\n",
        "    super(MyModel, self).__init__()\n",
        "    self._conv1a = tf.keras.layers.Conv2D(kernel_size=3, filters=16, padding='same', activation='relu')\n",
        "    self._conv1b = tf.keras.layers.Conv2D(kernel_size=3, filters=30, padding='same', activation='relu')\n",
        "    self._maxpool1 = tf.keras.layers.MaxPooling2D(pool_size=2)\n",
        "    \n",
        "    self._conv2a = tf.keras.layers.Conv2D(kernel_size=3, filters=60, padding='same', activation='relu')\n",
        "    self._maxpool2 = tf.keras.layers.MaxPooling2D(pool_size=2)\n",
        "    \n",
        "    self._conv3a = tf.keras.layers.Conv2D(kernel_size=3, filters=90, padding='same', activation='relu')\n",
        "    self._maxpool3 = tf.keras.layers.MaxPooling2D(pool_size=2)\n",
        "    \n",
        "    self._conv4a = tf.keras.layers.Conv2D(kernel_size=3, filters=110, padding='same', activation='relu')\n",
        "    self._maxpool4 = tf.keras.layers.MaxPooling2D(pool_size=2)\n",
        "    \n",
        "    self._conv5a = tf.keras.layers.Conv2D(kernel_size=3, filters=130, padding='same', activation='relu')\n",
        "    self._conv5b = tf.keras.layers.Conv2D(kernel_size=3, filters=40, padding='same', activation='relu')\n",
        "    \n",
        "    self._pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    self._classifier = tf.keras.layers.Dense(classes, activation='softmax')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self._conv1a(inputs)\n",
        "    x = self._conv1b(x)\n",
        "    x = self._maxpool1(x)\n",
        "\n",
        "    x = self._conv2a(x)\n",
        "    x = self._maxpool2(x)\n",
        "\n",
        "    x = self._conv3a(x)\n",
        "    x = self._maxpool3(x)\n",
        "\n",
        "    x = self._conv4a(x)\n",
        "    x = self._maxpool4(x)\n",
        "\n",
        "    x = self._conv5a(x)\n",
        "    x = self._conv5b(x)\n",
        "\n",
        "    x = self._pooling(x)\n",
        "    x = self._classifier(x)\n",
        "    return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN7YXbAVjHm-"
      },
      "source": [
        "with strategy.scope():\n",
        "  model = MyModel(classes=len(CLASSES))\n",
        "  # Set reduction to `none` so we can do the reduction afterwards and divide by\n",
        "  # global batch size.\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "  def compute_loss(labels, predictions):\n",
        "    per_example_loss = loss_object(labels, predictions)\n",
        "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZE * strategy.num_replicas_in_sync)\n",
        "\n",
        "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "      name='train_accuracy')\n",
        "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "      name='test_accuracy')\n",
        "  \n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "  @tf.function\n",
        "  def distributed_train_step(dataset_inputs):\n",
        "    per_replica_losses = strategy.run(train_step,args=(dataset_inputs,))\n",
        "    print(per_replica_losses)\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "                           axis=None)\n",
        " \n",
        "  @tf.function\n",
        "  def distributed_test_step(dataset_inputs):\n",
        "    strategy.run(test_step, args=(dataset_inputs,))\n",
        "\n",
        "\n",
        "  def train_step(inputs):\n",
        "    images, labels = inputs\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(images)\n",
        "      loss = compute_loss(labels, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_accuracy.update_state(labels, predictions)\n",
        "\n",
        "    return loss \n",
        "\n",
        "  def test_step(inputs):\n",
        "    images, labels = inputs\n",
        "\n",
        "    predictions = model(images)\n",
        "    loss = loss_object(labels, predictions)\n",
        "\n",
        "    test_loss.update_state(loss)\n",
        "    test_accuracy.update_state(labels, predictions)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGufHI5DjXVa"
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlIfImO8jQAn"
      },
      "source": [
        "EPOCHS = 40\n",
        "with strategy.scope():\n",
        "  for epoch in range(EPOCHS):\n",
        "    # TRAINING LOOP\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for x in get_training_dataset():\n",
        "      total_loss += distributed_train_step(x)\n",
        "      num_batches += 1\n",
        "    train_loss = total_loss / num_batches\n",
        "\n",
        "    # TESTING LOOP\n",
        "    for x in get_validation_dataset():\n",
        "      distributed_test_step(x)\n",
        "\n",
        "    template = (\"Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}, Test Loss: {:.2f}, \"\n",
        "                \"Test Accuracy: {:.2f}\")\n",
        "    print (template.format(epoch+1, train_loss,\n",
        "                           train_accuracy.result()*100, test_loss.result() / strategy.num_replicas_in_sync,\n",
        "                           test_accuracy.result()*100))\n",
        "\n",
        "    test_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_accuracy.reset_states()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dLLAOxMjgMf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}